# TrendyTopics_HistoriallinenAikakauskirja_2001-2013
Introduction to Methods in Digital Humanities course, University of Helsinki
This project was done as a part of Introduction to Methods for Digital Humanities course at the University of Helsinki.
In this project I will use topic modelling with Mallet to determine topics covered in Historiallinen aikakauskirja, which is a peer reviewed quarterly magazine publication. It is the considered to be one of the most important historical publications for researchers in Finland. The publication covers topics related to history and studying of history. This project will track what has been trendy in history studies in Finland 2001-2013 by finding the most popular topics of this publication.
It is important to point out early on that the results of this project are not reliable.
First, I downloaded the raw text files of the articles featured in Historiallinen Aikakauskirja in 2001-2013 from the Language Bank of Finland KORP’s Finnish Magazines and Newspapers from the 1990s and 2000s collection (http://metashare.csc.fi/repository/browse/corpus-of-finnish-magazines-and-newspapers-from-the-1990s-and-2000s-version-1/3a957d18b84611e5bfee005056be118ed9096cc20af04a1c8f794f1a6cec998a/). I combined the separate article files to a single file rawtext.txt as it seemed to make things simpler for this study as all of the text probably include more than one “main” topic (in the future, it would be useful to do topic modelling keeping the files intact, but that seemed to be too difficult for me now). I also left the text un-lemmatized and un-stemmed due to lack of skills.
The material was then cleaned up on Python (cleaning.py) by removing capital letters, line changes, punctuations and removing a personalized list of stopwords (list from here https://github.com/stopwords-iso/stopwords-fi, added words like historia, historiallinen, kolumni, yliopistonlehtori, dosentti, professori based on the data, full list is available in the directory as well as in the code (stopwordlist.txt). After doing the clean-up of the text, I wrote the results into one single file, my corpus, cleantext.txt. I could have done on Mallet, if I wanted to, but since I wanted to personalize the stopwords etc. I think I made the right choice with doing the data pre-processing beforehand.
Looking at the data after clean-up, it is evident that I had not been able to remove every “error”, there are still some weird blank spots etc. Also the fact that for answering my research question I needed to keep the numbers in the text to possibly find the most trendy decades etc but in the text there are some other number combinations that do not make any sense (related to the number of the article etc). I decided to look past all of that and see what the results were.
Then I imported the cleaned text to MALLET for train the system to do a basic topic modelling on the data. I trained the system to look for 50 topics. At this point, I noticed that despite the knowledge that Historiallinen aikakauskirja is a Finnish-language publication there have been multiple articles in English and in Swedish for those words appear on the keywords to each topic. I have also done some close-reading of the publication in the past and did not remember such language variance at all. The topics with their “keywords” (top_50_topics_historiallinen_aikakauskirja.exe) do not seem to make any sense and I was not able to draw out any sensible topics from them. Therefore, I think my project failed to reach its goal to answer the research question.
I could have cleaned the text better and left the files separate for more accurate results. After realizing that I also understood that I should have lemmatized my text but I didn’t have skills to do it. I also should have spotted the unexpected language differences within the raw data earlier and found the files and removed them from the data sample. I probably should have trained Mallet to look for more topics due to the mass on data (a lot of different topics and specialities) to get more accurate topics, but then I ran into some memory problems with Mallet that I could not fix (I kept getting new error messages each time I changed the amount of memory Mallet is permitted to use), I think my computer does not have enough space.
I then decided to run the cleaned text through Voyant-tools to reach for the most frequent words in the text. This experiment revealed some interesting results of the same material. If we look past the English words (see the visualisation-file historiallinen_aikakauskirja_most_frequent.png) we can see that the most frequent words include words related to Finland (suomen, suomessa, helsinki) (the bigger the font, the more frequent the word). The visualisation inludes also a few words related to Russia and the Soviet Union. This could indicate that most of the published articles relate to history related to the past of Finland (the neighbouring countries included). From this light frequency analysis, I do not dare to draw huge conclusions, for I do not believe that it is consistent enough. The words most frequent in the data do however seem to correlate with my analysis of the same material done with close-reading: I found out that most articles are tied to Finland in some way.
One could continue this research to actually make it work or to find out for example how the trends have changed in the years, to track emerging trends and to find out how the 90th Independence Day of Finland may have affected the published studies. That would require the researcher to find out which of the articles were published in which issues (that it not evident in the raw data), but I assume one skilled enough could use the pdf versions as well.
